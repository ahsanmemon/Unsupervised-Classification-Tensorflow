{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import sys\n",
    "import collections \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100, cifar10\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from scan_tf.models.train import pretext_training, pretext_training_bis, semantic_clustering_training \n",
    "from scan_tf.models.resnet import *\n",
    "import scan_tf.utils.utils as utils\n",
    "import scan_tf.utils.augmentations as augmentations\n",
    "\n",
    "\n",
    "# Grow memory to avoid mem overflow\n",
    "memory_growth=True\n",
    "if memory_growth:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      try:\n",
    "        for gpu in gpus:\n",
    "          tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "img_width, img_height, img_num_channels = 32, 32, 3\n",
    "no_epochs = 100\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "\n",
    "\n",
    "\n",
    "X_train.shape\n",
    "input_train = X_train\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Find number of classes dynamically\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "# Determine shape of the data\n",
    "input_shape = (img_width, img_height, img_num_channels)\n",
    "\n",
    "# Normalize data\n",
    "#X_train = (X_train/255).astype(np.float32)\n",
    "#X_test = (X_test/255).astype(np.float32)\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape : {X_train.shape}\")\n",
    "print(f\"y_test shape : {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = resnet_18(n_output=128)\n",
    "backbone_model.build(input_shape=(None, img_width, img_height, img_num_channels))\n",
    "backbone_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training for minimizing the Rotation Loss</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretext_model_save_path=\"pretext_task_rotnet_correct.h5\"\n",
    "train_pretext = True\n",
    "if train_pretext:\n",
    "    pretext_model = pretext_training_bis(backbone_model, X_train, y_train, epochs=50, save_path=pretext_model_save_path)\n",
    "else:\n",
    "    pretext_model = backbone_model\n",
    "    pretext_model.load_weights(pretext_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = X_train[:10]\n",
    "degrees_to_rotate = np.random.choice([180], 10).astype(int)\n",
    "rotated_images = np.array([scipy.ndimage.rotate(images[i], degrees_to_rotate[i], axes=(0, 1)) for i in range(len(degrees_to_rotate))])\n",
    "print(rotated_images.shape)\n",
    "for i in range(1):\n",
    "    plt.imshow(rotated_images[i].astype(\"uint8\"))\n",
    "    plt.show()\n",
    "    plt.imshow(images[i].astype(\"uint8\"))\n",
    "    plt.show()\n",
    "    plt.imshow(np.rot90(np.rot90(images[i].astype(\"uint8\"))))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Finding Nearest Neighbors (for debugging)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbor_consistancy(pretext_model, images, labels, n_neighbors=5, plot=False):\n",
    "    n_neighbors = 6\n",
    "    nn = utils.CLusteringNN(pretext_model, n_neighbors=n_neighbors)\n",
    "    nn.fit(images)\n",
    "    nn_indexes = nn.get_neighbors_indexes(images)\n",
    "    \n",
    "    n = 4\n",
    "    f, axes = plt.subplots(n, n_neighbors)\n",
    "    for i, cluster_indexes in enumerate(nn_indexes[:n]):\n",
    "        for j, im_i in enumerate(cluster_indexes):\n",
    "            axes[i,j].axis('off')\n",
    "            axes[i,j].imshow(images[im_i].astype(\"uint8\"))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    consistancies = list()\n",
    "    true_classes = np.argmax(labels, axis=1)\n",
    "    for cluster_indexes in nn_indexes:\n",
    "        cluster_classes = true_classes[cluster_indexes]\n",
    "        # Not sure which formula is better\n",
    "        consistancy = (cluster_classes[1:]==cluster_classes[0]).sum()/(len(cluster_indexes)-1)\n",
    "        # Not sure which formula is better\n",
    "        #cluster_label_counter = collections.Counter(cluster_classes)\n",
    "        #consistancy = cluster_label_counter.most_common()[0][1]/len(cluster_indexes)\n",
    "        consistancies.append(consistancy)\n",
    "    if plot:\n",
    "        sns.distplot(consistancies)\n",
    "        plt.xlabel(f\"Consistancy of {n_neighbors-1} nearest neighbors\")\n",
    "        plt.show()\n",
    "    return consistancies\n",
    "\n",
    "consistancies = find_neighbor_consistancy(pretext_model, X_train, y_train, n_neighbors=5, plot=True)\n",
    "print(f\"Correct Number of pairs: {np.mean(consistancies)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K means (can substitute semantic clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "train_embeddings = pretext_model.predict(X_train)\n",
    "kmeans = KMeans(n_clusters=y_train.shape[1], random_state=0).fit(train_embeddings)\n",
    "predicted_clusters = kmeans.predict(train_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_clustering_model_save_path=\"semantic_clustering_task.h5\"\n",
    "train_semantic_clustering = False\n",
    "if train_semantic_clustering:\n",
    "    semantic_clustering_model = semantic_clustering_training(pretext_model, X_train, y_train, epochs=20, save_path=semantic_clustering_model_save_path)\n",
    "else:\n",
    "    # Hope that num clusters is correct lol\n",
    "    num_clusters = y_train.shape[1]\n",
    "    semantic_clustering_model = add_classification_layer(pretext_model, num_clusters)\n",
    "    input_shape = (None,) + X_train.shape[1:]\n",
    "    semantic_clustering_model.build(input_shape)\n",
    "    semantic_clustering_model.load_weights(semantic_clustering_model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hungarian algorithm to match clusters with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_probability_associations = semantic_clustering_model.predict(X_train)\n",
    "predicted_clusters = np.argmax(cluster_probability_associations, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "# Run hungarian algorithm for matching\n",
    "true_labels = np.argmax(y_train, axis=1)\n",
    "plt.hist(true_labels, alpha=0.5, label=\"Label\")\n",
    "plt.hist(predicted_clusters, alpha=0.5, label=\"Cluster\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create cost matrix\n",
    "frequencies = np.zeros((np.unique(predicted_clusters).shape[0], y_train.shape[1]))\n",
    "for i, j in zip(predicted_clusters, true_labels):\n",
    "    frequencies[i,j] += 1\n",
    "cost_matrix = -frequencies\n",
    "\n",
    "# Run Hungarian algo to match clusters\n",
    "row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "for match in zip(row_ind, col_ind):\n",
    "    print(f\"Cluster {match[0]} matched with label {match[1]}\")\n",
    "\n",
    "#print(frequencies[col_ind][:, row_ind])\n",
    "correct_assigned = frequencies[row_ind, col_ind].sum()\n",
    "print(f\"Accuracy: {correct_assigned/np.sum(frequencies)*100:.2f}%\")\n",
    "predicted_labels = np.array(col_ind[np.where(row_ind==cluster)[0][0]] for cluster in predicted_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
